{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Import Library\n",
        "import joblib\n",
        "import re\n",
        "import nltk\n",
        "from textblob import TextBlob\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SHVOxJ8w84V",
        "outputId": "b64a8f4d-ce6a-4297-c6b4-d3f15b9e7e02"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xhgswJTMv5Tn"
      },
      "outputs": [],
      "source": [
        "# Preprocessing\n",
        "\n",
        "# Cleaning text\n",
        "def cleaningText(text):\n",
        "    text = re.sub(r'@[\\w]+', '', text)  # Menghapus mention\n",
        "    text = re.sub(r'#\\w+', '', text)  # Menghapus hashtag\n",
        "    text = re.sub(r'RT\\s', '', text)  # Menghapus RT\n",
        "    text = re.sub(r'http\\S+', '', text)  # Menghapus tautan (URL)\n",
        "    text = re.sub(r'\\d+', '', text)  # Menghapus angka\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Menghapus tanda baca\n",
        "    text = re.sub(r'\\n', ' ', text)  # Mengganti newline dengan spasi\n",
        "    text = text.strip()  # Menghapus spasi ekstra di awal dan akhir\n",
        "    return text\n",
        "\n",
        "# CaseFolding text\n",
        "def casefoldingText(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Tokenizing text\n",
        "def tokenizingText(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "# Filtering text\n",
        "def filteringText(tokens):\n",
        "    stopwords_list = set(nltk.corpus.stopwords.words('english'))\n",
        "    return [word for word in tokens if word.lower() not in stopwords_list]\n",
        "\n",
        "# Stemming text\n",
        "def stemmingText(tokens):\n",
        "    ps = nltk.PorterStemmer()\n",
        "    return [ps.stem(word) for word in tokens]\n",
        "\n",
        "# To Sentence\n",
        "def toSentence(tokens):\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Preprocessing\n",
        "def preprocess_input(user_input):\n",
        "    cleaned_text = cleaningText(user_input)\n",
        "    casefolded_text = casefoldingText(cleaned_text)\n",
        "    tokenized_text = tokenizingText(casefolded_text)\n",
        "    filtered_text = filteringText(tokenized_text)\n",
        "    stemmed_text = stemmingText(filtered_text)\n",
        "    final_text = toSentence(stemmed_text)\n",
        "    return final_text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model\n",
        "model = joblib.load('sentiment_model.pkl')\n",
        "vectorizer = joblib.load('tfidf_vectorizer.pkl')\n",
        "\n",
        "def predict_sentiment(user_input):\n",
        "    processed_input = preprocess_input(user_input)\n",
        "    input_tfidf = vectorizer.transform([processed_input])\n",
        "    sentiment_prediction = model.predict(input_tfidf)\n",
        "    return sentiment_prediction[0]"
      ],
      "metadata": {
        "id": "7SdtCWK_xKAk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input teks dari pengguna\n",
        "user_input = input(\"Enter text for sentiment analysis: \")\n",
        "\n",
        "# Prediksi sentimen\n",
        "sentiment = predict_sentiment(user_input)\n",
        "print(f\"The sentiment of the text is : {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZDGxVu8xHZu",
        "outputId": "0ce85572-5d94-4398-8d32-df51a2e68d11"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text for sentiment analysis: game is good\n",
            "The sentiment of the text is : positif\n"
          ]
        }
      ]
    }
  ]
}